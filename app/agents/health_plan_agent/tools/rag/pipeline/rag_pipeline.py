"""
pipeline.rag_pipeline

Main orchestration of the Retrieval-Augmented Generation (RAG) pipeline using LangGraph and LangChain Core Runnables.
1. Rewrite the input query for improved retrieval.
2. Retrieve relevant document chunks.
3. Generate the answer via the LLM chain.
"""

import time
from typing import List, Dict, Any, TypedDict

from langgraph.graph import StateGraph, START, END
from app.agents.health_plan_agent.tools.rag.pipeline.retriever import Retriever
from app.llm_factory import get_llm_provider
from app.agents.health_plan_agent.tools.rag.utils.callbacks import get_callback_manager
from app.agents.health_plan_agent.tools.rag.utils.logger import get_logger
from app.config import LANGSMITH_PROJECT
from langsmith import traceable

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable

from app.llm_factory import get_llm_provider
# … outras importações …

_llm_provider = None

def init_llm(llm_provider):
    """
    Inicializa o LLM a ser usado pelo pipeline RAG.
    Deve ser chamado a partir de app.py após a seleção do LLM.
    """
    global _llm_provider
    _llm_provider = llm_provider


# Define the shape of the graph state
class RAGState(TypedDict):
    query: str
    rewritten_query: str
    contexts: List[str]
    answer: str

class RAGPipeline:
    """
    Orchestrator for the Retrieval-Augmented Generation (RAG) pipeline,
    now leveraging LangGraph state graph and LangChain Core Runnables.
    """

    def __init__(self, k: int = 2) -> None:
        """
        Initialize the RAGPipeline, build LangChain chains, and LangGraph workflow.

        Parameters
        ----------
        k : int
            Number of top similar document chunks to retrieve (default=2).
        """
        self.k = k
        self.logger = get_logger(__name__)
        self.retriever = Retriever()
        self.llm = _llm_provider or get_llm_provider('openai')
        self.callback_manager = get_callback_manager()

        # -- LangChain: define prompt chains --
        rewrite_prompt = ChatPromptTemplate.from_messages([
            ("system", "Rewrite the query for better document retrieval."),
            ("human", "{query}")
        ])
        self.rewrite_chain: Runnable = rewrite_prompt | self.llm

        answer_prompt = ChatPromptTemplate.from_messages([
            ("system", "Use the following context to answer the question as completely and accurately as possible."),
            ("human", "Context:\n{contexts}\n\nQuestion: {query}\nAnswer:")
        ])
        self.answer_chain: Runnable = answer_prompt | self.llm

        # -- LangGraph: build workflow --
        self.workflow = StateGraph(RAGState)
        self.workflow.add_node("rewrite", self._rewrite_node)
        self.workflow.add_node("retrieve", self._retrieve_node)
        self.workflow.add_node("generate", self._generate_node)

        self.workflow.add_edge(START, "rewrite")
        self.workflow.add_edge("rewrite", "retrieve")
        self.workflow.add_edge("retrieve", "generate")
        self.workflow.add_edge("generate", END)

        self.app = self.workflow.compile()

    @traceable(project_name=LANGSMITH_PROJECT, name='Rag_Plano')
    def run(self, query: str) -> str:
        """
        Execute the RAG pipeline for the given query via LangGraph and LangChain.

        Parameters
        ----------
        query : str
            The user question to answer.

        Returns
        -------
        str
            The answer generated by the LLM.
        """
        self.logger.info("Starting RAG pipeline", extra={"query_length": len(query)})
        start_time = time.time()

        initial_state: RAGState = {
            "query": query,
            "rewritten_query": "",
            "contexts": [],
            "answer": ""
        }
        state = self.app.invoke(initial_state)
        print(self.llm)
        duration = time.time() - start_time
        self.logger.info(
            "RAG pipeline finished",
            extra={"total_duration_sec": duration, "answer_length": len(state["answer"])},
        )
        return state["answer"]

    def _rewrite_node(self, state: RAGState) -> Dict[str, Any]:
        # Use LangChain chain to rewrite the query
        result = self.rewrite_chain.invoke({"query": state["query"]})
        # Extract text content if returned as a message
        rewritten_text = result.content if hasattr(result, "content") else str(result)
        return {"rewritten_query": rewritten_text}

    def _retrieve_node(self, state: RAGState) -> Dict[str, Any]:
        # Retrieve document chunks using the rewritten query
        docs = self.retriever.retrieve(state["rewritten_query"], k=self.k)
        contents = [doc["content"] for doc in docs]
        return {"contexts": contents}

    def _generate_node(self, state: RAGState) -> Dict[str, Any]:
        # Flatten contexts and generate the answer via LLM chain
        contexts_str = "\n\n".join(state["contexts"])
        result = self.answer_chain.invoke({"contexts": contexts_str, "query": state["rewritten_query"]})
        answer_text = result.content if hasattr(result, "content") else str(result)
        return {"answer": answer_text}
